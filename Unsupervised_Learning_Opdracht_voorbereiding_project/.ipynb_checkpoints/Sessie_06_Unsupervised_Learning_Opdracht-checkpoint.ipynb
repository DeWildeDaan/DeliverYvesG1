{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import data, color, io, filters, morphology,transform, exposure, feature\n",
    "from scipy import ndimage\n",
    "from skimage.io import imread, imshow\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Clustering\n",
    "\n",
    "Een supermarkt verzamelde een aantal gegevens van zijn klanten. Deze gegevens wil de supermarkt gebruiken om zijn klanten beter in kaart te brengen. \n",
    "Daarvoor wil het clustering toepassen. Op deze manier worden klanten met een gelijkaardig profiel samen gegroepeerd. Dit maakt het mogelijk voor de supermarkt om per cluster van klanten doelgerichter promoties te kunnen aanbieden.\n",
    "\n",
    "De gegevens kan je vinden in het bestand 'customer.csv'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RackId</th>\n",
       "      <th>RackRow</th>\n",
       "      <th>WeightPre</th>\n",
       "      <th>WeightPost</th>\n",
       "      <th>WeightDiff</th>\n",
       "      <th>DistMin</th>\n",
       "      <th>DistMax</th>\n",
       "      <th>DistAvg</th>\n",
       "      <th>DistTime</th>\n",
       "      <th>DayTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>12/25/1924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>6:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>7:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>7:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RackId  RackRow  WeightPre  WeightPost  WeightDiff  DistMin  DistMax  \\\n",
       "0       3        1         60          50           2        1       40   \n",
       "1       2        2         38          80           3        1       40   \n",
       "2       3        1         40          50           1        1       40   \n",
       "3       2        2         88          80           5        1        1   \n",
       "4       1        2         65          80           1        1       40   \n",
       "\n",
       "   DistAvg  DistTime     DayTime  \n",
       "0       10         3  12/25/1924  \n",
       "1       19         4        6:00  \n",
       "2       23         4        6:30  \n",
       "3       40         6        7:00  \n",
       "4       28         2        7:01  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./sensors_data_test_project.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse van de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RackId</th>\n",
       "      <th>RackRow</th>\n",
       "      <th>WeightPre</th>\n",
       "      <th>WeightPost</th>\n",
       "      <th>WeightDiff</th>\n",
       "      <th>DistMin</th>\n",
       "      <th>DistMax</th>\n",
       "      <th>DistAvg</th>\n",
       "      <th>DistTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RackId  RackRow  WeightPre  WeightPost  WeightDiff  DistMin  DistMax  \\\n",
       "0       3        1         60          50           2        1       40   \n",
       "1       2        2         38          80           3        1       40   \n",
       "2       3        1         40          50           1        1       40   \n",
       "3       2        2         88          80           5        1        1   \n",
       "4       1        2         65          80           1        1       40   \n",
       "\n",
       "   DistAvg  DistTime  \n",
       "0       10         3  \n",
       "1       19         4  \n",
       "2       23         4  \n",
       "3       40         6  \n",
       "4       28         2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verwijderen van DayTime\n",
    "\n",
    "\n",
    "dataset.drop('DayTime',axis=1,inplace=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderzoek de verdeling per geslacht. Wat is de verhouding van mannelijke klanten ten opzichte van vrouwelijke klanten? Gebruik hiervoor de seaborn countplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verdeling van  DistAvg\n",
    "\n",
    "\n",
    "# Onderzoek in welke mate de dataset gebalanceerd is.\n",
    "\n",
    "sns.countplot(x='DistAvg',data=dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ga na of vrouwen meer geld lijken te spenderen dan mannen. Visualiseer dit met een boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot van spending volgens geslacht\n",
    "\n",
    "\n",
    "sns.boxplot(x='DistAvg',y='Spending_Score',data=dataset,orient='v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vervang male door 0 en female door 1\n",
    "dataset.replace({'Gender': {'Male': 0, 'Female': 1}},inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teken de histogrammen van Age, Income en Spending score. Je kan hiervoor de seaborn distplot gebruiken. Zijn deze normaal verdeeld? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributie van de Age, Income en Spending_Score\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(1 , figsize = (15 , 6))\n",
    "plt.subplot(1 , 3 , 1)\n",
    "sns.distplot(dataset['Age'],bins=15,axlabel='Age')\n",
    "plt.subplot(1 , 3 , 2)\n",
    "sns.distplot(dataset['Annual_Income'],bins=15,axlabel='Annual Income')\n",
    "plt.subplot(1 , 3 , 3)\n",
    "sns.distplot(dataset['Spending_Score'],bins=15,axlabel='Spending Score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot\n",
    "\n",
    "sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Tijd om op zoek te gaan naar een goede clustering van de klanten. \n",
    "Bij clustering is het belangrijk om de features eerst te normaliseren. Veel clustering technieken maken namelijk gebruik van een afstandsfunctie om de gelijkenis tussen verschillende datapunten te bepalen. Daarom is het noodzakelijk dat alle variabelen op een gelijke schaalverdeling staan. \n",
    "\n",
    "- Teken een scatterplot tussen Age en Spending_Score. \n",
    "- Hoeveel clusters denk je te zien in de scatterplot?\n",
    "- Schaal de features Age en Spending score\n",
    "- Pas K-means clutering toe. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html. Kies een K waarde (=aantal clusters) op basis van je schatting.\n",
    "- Visualiseer de clusters op de scatterplot\n",
    "- Ga nu op zoek naar het optimale aantal clusters gebruik makende van de elleboog methode. Hiervoor bereken je de totale gekwadrateerde afstand van de alle datapunten binnen een cluster tot het centrum van de cluster waartoe ze behoren. Dit wordt automatische berekend tijdens het trainen. Je kan deze waarde na training ophalen via de **inertia_** property. Maak een for-loop waarin je K varieert tussen 1 en 10. Voor elke K-waarde train je de K-means en visualiseer je de clusters. Hou ook telkens de inertia_ bij. Maak vervolgens een plot van de 'elleboog-curve' door de inertia_ uit te zetten in functie van K. Daar waar de knik (elleboog) zit is een goede indicatie voor het aantal te kiezen clusters.\n",
    "\n",
    "Herhaal bovenstaande voor de scatterplot tussen Annual_Income en Spending_Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Age en Spending_Score\n",
    "sns.scatterplot(x=dataset['Age'],y=dataset['Spending_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bepalen van het aantal clusters via de elleboog methode\n",
    "\n",
    "X_age = dataset[['Age' , 'Spending_Score']].iloc[: , :].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_age = scaler.fit_transform(X_age)\n",
    "\n",
    "\n",
    "\n",
    "inertia = []\n",
    "plt.figure(1 , figsize = (20 , 10))\n",
    "\n",
    "\n",
    "for c in range(1,11):\n",
    "    model = (KMeans(n_clusters = c ,init='k-means++', n_init = 10 ,max_iter=300, \n",
    "                        tol=0.0001,  random_state= 111  , algorithm='auto') )\n",
    "    model.fit(X_age)\n",
    "    y_kmeans = model.predict(X_age)\n",
    "    inertia.append(model.inertia_)\n",
    "    plt.subplot(2,5,c)\n",
    "    plt.scatter(X_age[:, 0], X_age[:, 1], c=y_kmeans, s=50, cmap='rainbow')\n",
    "    plt.xlabel('Aantal clusters: '+str(c))\n",
    "    \n",
    "\n",
    "\n",
    "plt.figure(2 , figsize = (20 , 6))\n",
    "\n",
    "plt.plot(np.arange(1 , 11) , inertia , 'o--',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot  Annual_Income / Spending_Score\n",
    "\n",
    "sns.scatterplot(x=dataset['Annual_Income'],y=dataset['Spending_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Annual_Income en Spending_Score\n",
    "\n",
    "X_income = dataset[['Annual_Income' , 'Spending_Score']].iloc[: , :].values\n",
    "inertia = []\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_income = scaler.fit_transform(X_income)\n",
    "\n",
    "plt.figure(1 , figsize = (20 , 10))\n",
    "\n",
    "\n",
    "for c in range(1,11):\n",
    "    model = (KMeans(n_clusters = c ,init='k-means++', n_init = 10 ,max_iter=300, \n",
    "                        tol=0.0001,  random_state= 111  , algorithm='auto') )\n",
    "    model.fit(X_income)\n",
    "    y_kmeans = model.predict(X_income)\n",
    "    inertia.append(model.inertia_)\n",
    "    plt.subplot(2,5,c)\n",
    "    plt.scatter(X_income[:, 0], X_income[:, 1], c=y_kmeans, s=50, cmap='rainbow')\n",
    "    plt.xlabel('Aantal clusters: '+str(c))\n",
    "    \n",
    "\n",
    "\n",
    "plt.figure(2 , figsize = (20 , 6))\n",
    "\n",
    "plt.plot(np.arange(1 , 11) , inertia , 'o--',color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probeer nu de scatterplot tussen Gender en Spending_Score te clusteren. Je zal merken dat er affiniteit bestaat tussen de datapunten. Een methode die geschikt is voor het clusteren van deze geconnecteerde data is spectral clustering. Pas deze toe op Annual_Income en Spending_Score\n",
    "Meer info over SpectralClustering is te vinden op https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot  Gender / Spending_Score\n",
    "\n",
    "sns.scatterplot(x=dataset['Gender'],y=dataset['Spending_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Gender en Spending_Score. Eerst via K-means daarna via Spectral Clustering\n",
    "\n",
    "# Bepalen van het aantal clusters via de elleboog methode\n",
    "\n",
    "X_gender = dataset[['Gender' , 'Spending_Score']].iloc[: , :].values\n",
    "inertia = []\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_gender = scaler.fit_transform(X_gender)\n",
    "\n",
    "\n",
    "plt.figure(1 , figsize = (20 , 10))\n",
    "\n",
    "\n",
    "for c in range(1,11):\n",
    "    model = KMeans(n_clusters = c ,init='k-means++', n_init = 10 ,max_iter=300, \n",
    "                        tol=0.0001,  random_state= 111  , algorithm='auto') \n",
    "    model.fit(X_gender)\n",
    "    y_kmeans = model.predict(X_gender)\n",
    "    inertia.append(model.inertia_)\n",
    "    plt.subplot(2,5,c)\n",
    "    plt.scatter(X_gender[:, 0], X_gender[:, 1], c=y_kmeans, s=50, cmap='rainbow')\n",
    "    plt.xlabel('Aantal clusters: '+str(c))\n",
    "    \n",
    "\n",
    "\n",
    "plt.figure(2 , figsize = (20 , 6))\n",
    "\n",
    "plt.plot(np.arange(1 , 11) , inertia , 'o--',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Je ziet dat K-means clustering hier niet goed genoeg is (omdat het te gevoelig is voor uitschieters(?)) dus spectral clustering genruiken\n",
    "#gebruiken (op basis van affiniteit)\n",
    "model_spectral = SpectralClustering(n_clusters=2)\n",
    "#model_spectral.fit(X_age)\n",
    "#y_spectral = model_spectral.labels_\n",
    "y_spectral = model_spectral.fit_predict(X_age)\n",
    "plt.figure(1 , figsize = (20 , 10))\n",
    "\n",
    "plt.scatter(X_age[:, 0], X_age[:, 1], c=y_spectral, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoek de clusters via K-means op basis van Age, Annual_Income en Spending_Score. Gebruik ook hier de elleboog methode om het aantal clusters te bepalen.\n",
    "\n",
    "Zet na clustering de data opniew terug naar de oorspronkelijk schaal. De meeste scalers beschikken over een inverse_transform methode die je kan gebruiken om terug te schalen.\n",
    "\n",
    "Plot de scatterplot en de clusters in 3D. Je bent vrij in de keuze van de library die je hiervoor gebruikt. \n",
    "\n",
    "In hoeveel clusters heb je de data ingedeeld? Kan je ze beschrijven? Welke zijn interessant voor de supermarkt om op te focussen willen ze hun winst verhogen?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering van Age, Annual_Income en Spending_Score + 3D visualisatie en uiteindelijke interpretatie\n",
    "X = dataset[['Age' , 'Annual_Income','Spending_Score']].iloc[: , :].values\n",
    "inertia = []\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "for c in range(1,11):\n",
    "    model = KMeans(n_clusters = c ,init='k-means++', n_init = 10 ,max_iter=300, \n",
    "                        tol=0.0001,  random_state= 111  , algorithm='auto') \n",
    "    model.fit(X)\n",
    "    y_kmeans = model.predict(X)\n",
    "    inertia.append(model.inertia_)\n",
    "       \n",
    "\n",
    "\n",
    "plt.figure(2 , figsize = (20 , 6))\n",
    "\n",
    "plt.plot(np.arange(1 , 11) , inertia , 'o--',color='red')\n",
    "\n",
    "\n",
    "model = KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, tol=0.0001,  random_state= 111  , algorithm='auto') \n",
    "model.fit(X)\n",
    "y_kmeans = model.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "xs = X[:,0]\n",
    "ys = X[:,1]\n",
    "zs = X[:,2]\n",
    "ax.scatter(xs, ys, zs, s=70, alpha=0.6, edgecolors='w', c=y_kmeans, cmap='rainbow')\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Annual_Income')\n",
    "ax.set_zlabel('Spending_Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Face recognition\n",
    "\n",
    "Het doel van deze opdracht bestaat erin om een zo performant mogelijke gezichtsherkenning uit te voeren aan de hand van Eigenfaces. Deze techniek van Eigenfaces was een van de eerste succesvolle pogingen om via computer vision gezichten te classificeren. Tegenwoordig is deze methode achterhaald en mijlenver voorbij gestoken door technieken gebaseerd op Convolutional Neural Networks (zie module deep learning).\n",
    "Bij deze opdracht worden een training- en test set van gezichten meegeleverd. De gezichten zijn afkomstig uit de Georgia Tech face database (http://www.anefian.com/research/face_reco.htm) en bevatten gezichten van 50 verschillende personen. Van elke persoon werden 15 foto's genomen.\n",
    "De foto's die ter beschikken worden gesteld ondergingen al enige vorm van preprocessing zoals face cropping en face rotation.\n",
    "Uit de bestandsnaam van elke foto kan het persoons ID worden gehaald. Voorbeeld: person22\\_15\\.jpg. Hier gaat het om de 15de foto van persoon 22.\n",
    "\n",
    "Deze opdracht is in heel sterke mate gelijklopend met de demo MNIST_PCA.\n",
    "\n",
    "De stappen om tot een succesvolle realisatie te komen zijn als volgt:\n",
    "- Inlezen van de afbeeldingen. Haal het label van het gezicht uit de bestandsnaam.\n",
    "- Converteer de afbeeldingen naar grijswaarden. Dit wordt reeds gedaan in de beschikbare code tijdens het inlezen van de afbeeldingen. \n",
    "- Schaal alle afbeeldingen naar dezelfde afmeting. Bijvoorbeeld 150x110 (150 rijen, 110 kolommen). Dit kan via skimage.transform.resize().\n",
    "- Transformeer de training en test set zodanig dat een individuele afbeelding bestaat uit 1 rij. Met andere woorden maak van een 2D afbeelding een 1D vector door alle rijen van de afbeelding achter elkaar te plakken. Dit kan via np.reshape().\n",
    "- Doe Principle Component Analyse. In de praktijk gebruikt men doorgaans een 40-tal Eigenfaces. Dit is ook het aantal wat je in eerste instantie mag gebruiken. Later wordt gevraagd dit aantal te variëren.\n",
    "- Train een classifier (logistic regression, SVM, Random Forest Classifier) op de principle components.\n",
    "- Test de classifier en evalueer de performantie (accuracy, recall, precision). Argumenteer waarom je voor een bepaalde classifier zou kiezen. \n",
    "- Doe hyperparameter tuning en probeer verschillende aantallen Eigenfaces.\n",
    "\n",
    "\n",
    "Als uitbreiding wordt gevraagd om een face detector te bouwen. In tegenstelling tot gezichtsherkenning is het niet de bedoeling het gezicht in de afbeelding te classificeren maar om de locatie van het gezicht in de afbeelding via een bounding box te kunnen aanduiden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bepalen van de principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inlezen van de training images\n",
    "training_images = [] # empty list\n",
    "test_images = [] # empty list\n",
    "y_train = [] # empty list\n",
    "y_test = [] # empty list\n",
    "\n",
    "path = \"./Faces/Training_images/\"\n",
    "valid_images = [\".jpg\",\".gif\",\".png\"]\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    training_images.append(imread(os.path.join(path,f),as_gray=True))\n",
    "    y_train.append(int(f[6:8]))\n",
    "   \n",
    "     \n",
    "# Inlezen van de test images\n",
    "path = \"./Faces/Test_images/\"\n",
    "valid_images = [\".jpg\",\".gif\",\".png\"]\n",
    "for f in os.listdir(path):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    test_images.append(imread(os.path.join(path,f),as_gray=True))\n",
    "    y_test.append(int(f[6:8]))\n",
    "    \n",
    "print(\"aantal training images: \", len(y_train))\n",
    "print(\"aantal test images: \", len(y_test))\n",
    "\n",
    "print(\"aantal unieke personen in de training set: \", len(set(y_train)))\n",
    "print(\"aantal unieke personen in de test set: \", len(set(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonen van een afbeelding ter controle via plt.imshow(image,cmap='gray')\n",
    "\n",
    "\n",
    "\n",
    "image_index = 1\n",
    "plt.imshow(training_images[image_index],cmap='gray')\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afbeelding herschalen naar afbeeldingen van 150 rijen en 110 kolommen\n",
    "# Skimage transform bevat de methode transform\n",
    "# transform.resize(image,(rows,cols),mode='constant')\n",
    "\n",
    "# Afbeelding herschalen naar afbeeldingen van 150 rijen en 110 kolommen\n",
    "rows = 150\n",
    "cols = 100\n",
    "\n",
    "for i in range(0,len(training_images)):\n",
    "    training_images[i] = transform.resize(training_images[i],(150,110),mode='constant')\n",
    "    \n",
    "for i in range(0,len(test_images)):\n",
    "    test_images[i]= transform.resize(test_images[i],(150,110),mode='constant')\n",
    "\n",
    "plt.imshow(training_images[image_index],cmap='gray')\n",
    "plt.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omzetten naar training en test set. Afbeeldingen naar 1 dimensie brengen (1 rij pixels per afbeelding)\n",
    "\n",
    "training_images = np.asarray(training_images)\n",
    "test_images = np.asarray(test_images)\n",
    "\n",
    "X_train = np.reshape(training_images,(len(training_images),-1))\n",
    "X_test = np.reshape(test_images,(len(test_images),-1))\n",
    "# Omzetten van list van labels naar een array van labels\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print(training_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction: Principle Component Analysis met 40 componenten\n",
    "\n",
    "number_of_components = 40\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Train het PCA algoritme op de training data\n",
    "pca_model = PCA(n_components=number_of_components, svd_solver='full')\n",
    "pca_model.fit(X_train)\n",
    "\n",
    "#Reduceer het aantal dimensies van zowel de trainig set als de test set\n",
    "X_train_pca = pca_model.transform(X_train)\n",
    "X_test_pca = pca_model.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer de eerste 10 principle components (de 10 eerste Eigenfaces)\n",
    "\n",
    "PCA_components = pca_model.components_\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9)) \n",
    "for i in range(0,10):\n",
    "    ax = fig.add_subplot(2, 5, i+1)\n",
    "    imshow(PCA_components[i].reshape((150,110)),cmap='gray')\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    plt.axis('off')\n",
    "    plt.legend('off')\n",
    "    \n",
    "    ax.set_title('Principle component'+str(i+1))\n",
    "    # x and y axis should be equal length\n",
    "    x0,x1 = ax.get_xlim()\n",
    "    y0,y1 = ax.get_ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geef de PCA scores van een bepaald gezicht\n",
    "# Wat is de betekenis van deze scores?\n",
    "\n",
    "print(X_train_pca[image_index,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geef weer hoeveel procent van de variantie in totaal wordt verklaard door alle principle components\n",
    "\n",
    "print(pca_model.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "print('\\n Totale verklaarde variantie: ', np.sum(pca_model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereer een gecombineerde grafiek van de verklaarde variantie in functie van de principle component. \n",
    "# Staafdiagram van de verklaarde variantie van de individuele principle componenten en anderzijds de cumulatieve\n",
    "# verklaarde variantie\n",
    "\n",
    "PCAnumber = np.arange(1,number_of_components+1)\n",
    "print(PCAnumber)\n",
    "PCA_explained_cumulative = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "print(PCA_explained_cumulative)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9)) \n",
    "ax = sns.barplot(PCAnumber,pca_model.explained_variance_ratio_,color='blue')\n",
    "plt.xlabel('PCA component')\n",
    "ax2=ax.twinx() # dubbele y-as\n",
    "ax.yaxis.set_label_position('left')\n",
    "ax2.yaxis.set_label_position('right')\n",
    "ax2.set_ylabel('Cumulatieve frequentie')\n",
    "plt.plot(PCA_explained_cumulative, c='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstrueer een gezicht aan de hand van de Eigenfaces\n",
    "image_reconstructed = pca_model.inverse_transform(X_train_pca[image_index,:])\n",
    "print(image_reconstructed.shape)\n",
    "\n",
    "plt.imshow(image_reconstructed.reshape((150, 110)),cmap = 'gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificatie\n",
    "\n",
    "Train meerdere classifiers (logistic regression, SVM, Random Forest Trees, Ensembles) voor het herkennen van de gezichten. Gebruik als features de gewichtsfactoren bekomen via de PCA analyse. Doe cross-validation voor het vinden van de optimale hyperparameters.\n",
    "\n",
    "- Varieer eens het aantal gebruikte Eigenfaces. Wat gebeurt er met de accuracy van de classifiers wanneer je het aantal vermindert (minder dan 40) en wat als je ze vermeerdert.\n",
    "\n",
    "- Heeft het aantal gebruikte Eigenfaces een sterke invloed op de rekentijd?\n",
    "\n",
    "- Beschrijf duidelijk welke methode je verkiest.\n",
    "\n",
    "- Visualiseer een aantal verkeerd geclassificeerde gezichten. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling van de eigenfaces\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_pca)\n",
    "X_train_pca = scaler.transform(X_train_pca)\n",
    "X_test_pca = scaler.transform(X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificatie\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint \n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Trainen van SVM via cross-validation\n",
    "\n",
    "model = SVC()\n",
    "parameters = {'kernel': ['linear','rbf','poly'],\n",
    "              'C': uniform(0.01, 10000), # haal C uit een random uniform distribution\n",
    "              'gamma': uniform(0.001, 3)}\n",
    "\n",
    "\n",
    "n_iter_search = 500\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=5,n_iter=n_iter_search,n_jobs = -1,verbose=1,random_state=0)\n",
    "\n",
    "random_search = random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', random_search.best_score_)\n",
    "print('Best parameters :',random_search.best_params_  )\n",
    "\n",
    "y_pred = random_search.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation via random search\n",
    "\n",
    "model = LogisticRegression()\n",
    "parameters = {'C': uniform(0.01, 10000), 'solver':['liblinear','lbfgs'],'penalty':['l1','l2']}\n",
    "\n",
    "\n",
    "n_iter_search = 1000\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=5,n_iter=n_iter_search,n_jobs = -1,verbose=5,random_state=0)\n",
    "\n",
    "random_search = random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', random_search.best_score_)\n",
    "print('Best parameters :',random_search.best_params_  )\n",
    "\n",
    "y_pred = random_search.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verkeerd geklassificeerde gezichten:\n",
    "\n",
    "print(y_pred!=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(training_images[2],cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xgboost in een one-vs-rest configuratie\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = OneVsRestClassifier(XGBClassifier(n_jobs=-1, max_depth=15))\n",
    "\n",
    "\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adiboost\n",
    "model_boost = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=15),\n",
    "    n_estimators=200,\n",
    "    learning_rate=2)\n",
    "\n",
    "\n",
    "model_boost.fit(X_train_pca,y_train)\n",
    "\n",
    "y_pred = model_boost.predict(X_test_pca)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting\n",
    "model_boost = LogisticRegression(C=100)\n",
    "\n",
    "\n",
    "model_boost.fit(X_train_pca,y_train)\n",
    "\n",
    "y_pred = model_boost.predict(X_test_pca)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_test, y_pred) * 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kijk of je met ensemble methodes zoals boosting de accuracy nog kan verhogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble methodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Uitbreiding - Face detection\n",
    "\n",
    "De bedoeling van face detection is het kunnen lokaliseren van gezichten in een afbeelding. Tegenwoordig bestaan hiervoor bestaan speciale getrainde neurale netwerken die hier bijzonder goed in zijn die later in deep learning aan bod zullen komen.\n",
    "Wat deze opdracht gaan we een minder geavanceerde werkwijze hanteren:\n",
    "\n",
    "- Maak een training set die bestaat uit foto's van gezichten enerzijds maar ook uit foto's van niet gezichten anderzijds.Online zijn verschillende gezichtsdatasets te vinden. Bijvoorbeeld: https://lionbridge.ai/datasets/5-million-faces-top-15-free-image-datasets-for-facial-recognition/ en http://shuoyang1213.me/WIDERFACE/. Dataset met afbeeldingen van niet-gezichten zijn vrij makkelijk te vinden. Je kan ook enkel hoge resolutie afbeeldingen verzamelen waarop geen gezichten staan. Hieruit kan je heel wat subafbeeldingen knippen die je gebruikt voor de negatieve klasse.\n",
    "- Train een classifier (naar eigen keuze) die met een zo hoog mogelijke accuraatheid gezichten van niet-gezichten kan onderscheiden.\n",
    "- Test deze classifier om een idee te hebben van de classifiation accuracy. \n",
    "- Indien de performantie voldoende hoog is kan je een sliding window detector implementeren. Dit betekent dat je de test-afbeelding meerdere keren afscant met een sliding window van verschillende groottes. De classifier zal voor elke positie van het sliding window een classificatie doen van de subimage die onder het window is gelegen. Teken een bounding box wanneer de subimage een gezicht bevat.\n",
    "\n",
    "Voor het tekenen van een bounding box kan de volgende code gebruikt worden:\n",
    "\n",
    "```python\n",
    "def rectangle_perimeter(r0, c0, width, height, shape=None, clip=False):\n",
    "    rr, cc = [r0, r0 + width, r0 + width, r0], [c0, c0, c0 + height, c0 + height]\n",
    "    return skimage.draw.polygon_perimeter(rr, cc, shape=shape, clip=clip)\n",
    "\n",
    "# drawing the bounding box:\n",
    "rr, cc = rectangle_perimeter(y, x, w, w)\n",
    "image_detected[rr,cc] =255\n",
    "\n",
    "```\n",
    "\n",
    "Test de face detector op een afbeelding waarop 1 of meerdere personen staan afgebeeld en kijk of de gezichten worden gedetecteerd. \n",
    "\n",
    "Een voorbeeld:\n",
    "\n",
    "![alt text](./Jupyter_Images/Face_Detection.png) \n",
    "\n",
    "\n",
    "Extra uitbreiding:\n",
    "\n",
    "Probleem: omdat we de afbeelding meerdere keren afscannen, telkens met windows van verschillende grootte is het goed mogelijk dat er meerdere bounding boxes worden geteked rond hetzelfde gezicht. Zoek naar een manier om meerdere bounding boxes die bij hetzelfde gezicht horen samen te voegen tot 1 bounding box.\n",
    "Een veel gebruikte techniek is Non-Maximum Suppression: https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/ en https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uitwerking face detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
